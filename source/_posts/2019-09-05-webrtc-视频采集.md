---
title:      "WebRTC的视频采集(一)" 
date:       2019-09-05 22:00:00
tags:
    - WebRTC学习
categories:
    - WebRTC学习
---

路径`Source/sdk/objc/components/capture`

### RTCCameraVideoCapturer

- [RTCVideoCapturer](#RTCVideoCapturer)
- [RTCCameraVideoCapturer](#RTCCameraVideoCapturer)

<p id='RTCVideoCapturer'></p>
### `RTCVideoCapturer`

`RTCVideoCapturer`继承自`NSObject`，主要提供了一个视频采集回调的代理方法。

代理属性

`@property(nonatomic, weak) id<RTCVideoCapturerDelegate> delegate;`

初始化方法

`- (instancetype)initWithDelegate:(id<RTCVideoCapturerDelegate>)delegate;`

`<RTCVideoCapturerDelegate>`代理中有一个必须执行的方法

`- (void)capturer:(RTCVideoCapturer *)capturer didCaptureVideoFrame:(RTCVideoFrame *)frame;`

---
<p id='RTCCameraVideoCapturer'></p>
### `RTCCameraVideoCapturer `

`RTCCameraVideoCapturer`继承自[RTCVideoCapturer](#RTCVideoCapturer)，我理解为摄像头视频采集，可能还会有录屏采集、文件采集等其他子类。

只有一个只读属性`AVCaptureSession *captureSession`。

`+ (NSArray<AVCaptureDevice *> *)captureDevices;`类方法，返回可用的设备列表。

`+ (NSArray<AVCaptureDeviceFormat *> *)supportedFormatsForDevice:(AVCaptureDevice *)device;`根据设备返回支持的`AVCaptureDeviceFormat *`类型的列表

`- (FourCharCode)preferredOutputPixelFormat;`返回本采集器最有效支持的输出像素格式。

然后是两个开始采集和停止采集的方法，均为**异步**，稍后详细讨论这几个方法。

>讨论1

`@property`分别对应`@synthesize`和`@dynamic`。默认为`@synthesize`,为属性生成一个`_propertyName`的成员变量，并自动生成setter和getter方法。

`@synthesize`为属性声明一个非`_propertyName`的成员变量，之后使用`_propertyName`会报错。

`protocol`中声明的属性需要使用`@synthesize`，因为协议不会自动为属性声明setter和getter方法

`@dynamic var`用于告诉编译器，我要自己声明setter和getter方法，不需要自动生成。

```
- (instancetype)initWithDelegate:(__weak id<RTCVideoCapturerDelegate>)delegate
                  captureSession:(AVCaptureSession *)captureSession
```

最终的初始化方法，在此方法中添加了一些监听事件。

 - `UIDeviceOrientationDidChangeNotification`设备方向发生变化

 - `AVCaptureSessionWasInterruptedNotification`音视频采集已经（开始）被打断

 - `AVCaptureSessionInterruptionEndedNotification`音视频采集打断结束

 - `UIApplicationDidBecomeActiveNotification`应用变为激活状态

 - `AVCaptureSessionRuntimeErrorNotification`AVCaptureSession实例正在运行时发生未知错误

 - `AVCaptureSessionDidStartRunningNotification`AVCaptureSession实例成功开始运行

 - `AVCaptureSessionDidStopRunningNotification`AVCaptureSession实例停止运行


```
- (dispatch_queue_t)frameQueue {
  if (!_frameQueue) {
    _frameQueue =
        dispatch_queue_create("org.webrtc.cameravideocapturer.video", DISPATCH_QUEUE_SERIAL);
    dispatch_set_target_queue(_frameQueue,
                              dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_HIGH, 0));
  }
  return _frameQueue;
}
```
采集队列为串行队列，并且将优先级设置为高。

`<AVCaptureVideoDataOutputSampleBufferDelegate>`代理


```
/**
* 当AVCaptureVideoDataOutput的实例给出一个新的视频帧时会调用这个方法
* output：对应的给出视频帧的AVCaptureVideoDataOutput的实例对象
* sampleBuffer：一个CMSampleBuffer对象，包含视频帧数据和其他信息，比如编码和时间戳
*/
- (void)captureOutput:(AVCaptureOutput *)captureOutput
    didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
           fromConnection:(AVCaptureConnection *)connection {
           
  //判断当前output是否与预期的_videoDataOutput一致
  NSParameterAssert(captureOutput == _videoDataOutput);

  //CMSampleBufferGetNumSamples获取媒体样本数，返回0即有错误
  //CMSampleBufferIsValid判断sbuf是否可用
  //CMSampleBufferDataIsReady判断sbuf的数据是否准备好
  if (CMSampleBufferGetNumSamples(sampleBuffer) != 1 || !CMSampleBufferIsValid(sampleBuffer) ||
      !CMSampleBufferDataIsReady(sampleBuffer)) {
    return;
  }

  //将sampleBuffer转为imageBuffer，CVPixelBufferRef为CVImageBufferRef的别名
  CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
  if (pixelBuffer == nil) {
    return;
  }

#if TARGET_OS_IPHONE 
  BOOL usingFrontCamera = NO;
  // Check the image's EXIF for the camera the image came from as the image could have been
  // delayed as we set alwaysDiscardsLateVideoFrames to NO.
  //根据sampleBuffer获取照相机的方向，判断是否使用前置摄像头
  AVCaptureDevicePosition cameraPosition =
      [AVCaptureSession devicePositionForSampleBuffer:sampleBuffer];
  if (cameraPosition != AVCaptureDevicePositionUnspecified) {
    usingFrontCamera = AVCaptureDevicePositionFront == cameraPosition;
  } else {
    AVCaptureDeviceInput *deviceInput =
        (AVCaptureDeviceInput *)((AVCaptureInputPort *)connection.inputPorts.firstObject).input;
    usingFrontCamera = AVCaptureDevicePositionFront == deviceInput.device.position;
  }
  switch (_orientation) {
    case UIDeviceOrientationPortrait:
      _rotation = RTCVideoRotation_90;
      break;
    case UIDeviceOrientationPortraitUpsideDown:
      _rotation = RTCVideoRotation_270;
      break;
    case UIDeviceOrientationLandscapeLeft:
      _rotation = usingFrontCamera ? RTCVideoRotation_180 : RTCVideoRotation_0;
      break;
    case UIDeviceOrientationLandscapeRight:
      _rotation = usingFrontCamera ? RTCVideoRotation_0 : RTCVideoRotation_180;
      break;
    case UIDeviceOrientationFaceUp:
    case UIDeviceOrientationFaceDown:
    case UIDeviceOrientationUnknown:
      // Ignore.
      break;
  }
#else
  // No rotation on Mac.
  _rotation = RTCVideoRotation_0;
#endif
  //将imageBuffer转为RTC的RTCCVPixelBuffer
  RTCCVPixelBuffer *rtcPixelBuffer = [[RTCCVPixelBuffer alloc] initWithPixelBuffer:pixelBuffer];
  
  //获取时间戳
  int64_t timeStampNs = CMTimeGetSeconds(CMSampleBufferGetPresentationTimeStamp(sampleBuffer)) *
      kNanosecondsPerSecond;
  
  //根据rtcPixelBuffer、方向和时间戳构造RTCVideoFrame 并回调给代理
  RTCVideoFrame *videoFrame = [[RTCVideoFrame alloc] initWithBuffer:rtcPixelBuffer
                                                           rotation:_rotation
                                                        timeStampNs:timeStampNs];
  [self.delegate capturer:self didCaptureVideoFrame:videoFrame];
}
```

```
/**
* 当有一帧被丢弃时调用
* sampleBuffer：被丢弃帧的元数据，比如时常和时间戳，不包含实际的视频数据
*/
- (void)captureOutput:(AVCaptureOutput *)output didDropSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection;
```